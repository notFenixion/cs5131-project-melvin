{"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNiH+w4lMX2cLW90N81OzR9","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":924245,"sourceType":"datasetVersion","datasetId":464091},{"sourceId":3134515,"sourceType":"datasetVersion","datasetId":1909705},{"sourceId":7859636,"sourceType":"datasetVersion","datasetId":4610376},{"sourceId":7859653,"sourceType":"datasetVersion","datasetId":4610385}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Identification of Deepfaked Images (and Videos?)\n## By Li Run & Rongyi","metadata":{"id":"aivnid-yA_wd"}},{"cell_type":"markdown","source":"## Problem Statement","metadata":{}},{"cell_type":"markdown","source":"Within the past year, deepfaked media has risen to prominence all over the world. With it being near-impossible to differentiate between real and fake online nowadays, how can we help the average person tell what is real?\n\nThus the question arises: **Given an image, is it possible to tell if it is deepfaked or not?**\n\nIn this project, our aim is to develop an AI model that is capable of **identifying deepfaked images with â‰¥70% accuracy.**\n","metadata":{"id":"ROLraV2PBFe3"}},{"cell_type":"markdown","source":"## Data Collection\n\nWe utilized two datasets of images for training our model:\n1. https://www.kaggle.com/datasets/manjilkarki/deepfake-and-real-images\n2. https://www.kaggle.com/datasets/dagnelies/deepfake-faces\n\nThe first dataset contains approximately 70,000 training images, 5400 test images and 20,000 validation images of faces for both Real and Fake images each.\n\nThe second dataset contains approximately 95,600 images of faces. Labelling of the images as real or fake can be found under `metadata.csv`.","metadata":{"id":"D-etTf5cCbxy"}},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{"id":"1Rd_xm2QAvzV"}},{"cell_type":"markdown","source":"Let us first inspect the contents of `deepfake_faces`.","metadata":{"id":"Nof6xTiWEaR1"}},{"cell_type":"code","source":"# Uncomment line below to install tensorflow with cuda \n# !pip install tensorflow[and-cuda] --target=/kaggle/working/cuda-files","metadata":{"execution":{"iopub.status.busy":"2024-03-18T14:39:46.354937Z","iopub.execute_input":"2024-03-18T14:39:46.355292Z","iopub.status.idle":"2024-03-18T14:39:46.359404Z","shell.execute_reply.started":"2024-03-18T14:39:46.355263Z","shell.execute_reply":"2024-03-18T14:39:46.358453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imports\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nimport random\nfrom sklearn.utils.class_weight import compute_class_weight\nimport os, os.path, shutil\nfrom tqdm import tqdm\nimport cv2\nimport keras","metadata":{"id":"pkzydyrlhWaO","execution":{"iopub.status.busy":"2024-03-18T14:39:49.540456Z","iopub.execute_input":"2024-03-18T14:39:49.540818Z","iopub.status.idle":"2024-03-18T14:39:59.258301Z","shell.execute_reply.started":"2024-03-18T14:39:49.540791Z","shell.execute_reply":"2024-03-18T14:39:59.257318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/deepfake-faces/metadata.csv')\ndf.head()","metadata":{"id":"urOb3W5fhY40","outputId":"5279233e-68b7-42bb-c6b0-70cf452ebdb8","execution":{"iopub.status.busy":"2024-03-18T13:16:11.458260Z","iopub.execute_input":"2024-03-18T13:16:11.459016Z","iopub.status.idle":"2024-03-18T13:16:11.658561Z","shell.execute_reply.started":"2024-03-18T13:16:11.458984Z","shell.execute_reply":"2024-03-18T13:16:11.657428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.videoname == 'aaagqkcdis.mp4']","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:16:11.664623Z","iopub.execute_input":"2024-03-18T13:16:11.664921Z","iopub.status.idle":"2024-03-18T13:16:11.699159Z","shell.execute_reply.started":"2024-03-18T13:16:11.664888Z","shell.execute_reply":"2024-03-18T13:16:11.698210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We took the name of the first image `aaaqgkcdis.jpg` and looked it up in`metadata.csv`, confirming that the image names corresponded to entries within the csv file allowing us to label the images ourselves.","metadata":{}},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"id":"gH6n-ZA3U-ZF","outputId":"c0e0d2f2-f4fe-4a03-cb7f-288fd559093b","execution":{"iopub.status.busy":"2024-03-18T13:16:13.220064Z","iopub.execute_input":"2024-03-18T13:16:13.220488Z","iopub.status.idle":"2024-03-18T13:16:13.246967Z","shell.execute_reply.started":"2024-03-18T13:16:13.220455Z","shell.execute_reply":"2024-03-18T13:16:13.246063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From here we can see that the Fake:Real ratio in `deepfake_faces` is about 5:1. We need to handle this class imbalance in our data, which we will do by just taking a sample of 16,000 images from each Fake and Real instead. We also needs to categorise the images since they have not been labelled in the same format as in `deepfake-and-real-images`.\n","metadata":{"id":"MaqzHZShW2H1"}},{"cell_type":"code","source":"# NOTE: Run if output working folder is still empty\n\nFOLDER_PATH = '/kaggle/input/deepfake-faces/faces_224/'\nFAKE_PATH = '/kaggle/working/deepfake-faces/Fake'\nREAL_PATH = '/kaggle/working/deepfake-faces/Real'\n\nos.makedirs(FAKE_PATH, exist_ok=True)\nos.makedirs(REAL_PATH, exist_ok=True)   \n\nrealcount = 0\nfakecount = 0\nfor index, row in tqdm(df.iterrows()):\n    img_name = row['videoname'].split('.')[0] + '.jpg'\n    old_path = FOLDER_PATH + img_name\n    if row['label'] == 'REAL':\n        if realcount < 16000:\n            new_path = os.path.join(REAL_PATH', img_name)\n            realcount += 1\n        else:\n            continue\n\n    else:\n        if fakecount < 16000:\n            new_path = os.path.join(FAKE_PATH, img_name)\n            fakecount += 1\n        else:\n            continue\n    \n    shutil.copy(old_path, new_path)\n\n    \nprint(\"Categorisation complete\")\n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-17T11:55:00.452535Z","iopub.execute_input":"2024-03-17T11:55:00.452856Z","iopub.status.idle":"2024-03-17T11:59:56.991287Z","shell.execute_reply.started":"2024-03-17T11:55:00.452820Z","shell.execute_reply":"2024-03-17T11:59:56.990399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us test if our categorisation worked.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nfile_names = os.listdir(FAKE_PATH + '/')\nfor i in range(25):\n    idx = random.randint(0, len(file_names)) #take a random batch out of all the fake images\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    \n    \n    video_name = file_names[idx][:-4] + '.mp4'\n    plt.imshow(cv2.imread(os.path.join(FAKE_PATH, file_names[idx]))\n    # Redundant but i want to test if any real images made it in by some miracle\n    if(df[df.videoname == video_name].iloc[0]['label']=='FAKE'):\n        plt.xlabel('FAKE Image')\n    else:\n        plt.xlabel('REAL Image')\n        \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-17T11:59:56.992552Z","iopub.execute_input":"2024-03-17T11:59:56.992920Z","iopub.status.idle":"2024-03-17T11:59:59.358795Z","shell.execute_reply.started":"2024-03-17T11:59:56.992886Z","shell.execute_reply":"2024-03-17T11:59:59.357781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nfile_names = os.listdir(REAL_PATH + '/')\nfor i in range(25):\n    idx = random.randint(0, len(file_names)) #take a random batch out of all the fake images\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    \n    \n    video_name = file_names[idx][:-4] + '.mp4'\n    plt.imshow(cv2.imread(os.path.join(REAL_PATH, file_names[idx])))\n    # Redundant but i want to test if any fake images made it in by some miracle\n    if(df[df.videoname == video_name].iloc[0]['label']=='FAKE'):\n        plt.xlabel('FAKE Image')\n    else:\n        plt.xlabel('REAL Image')\n        \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-17T11:59:59.360096Z","iopub.execute_input":"2024-03-17T11:59:59.360396Z","iopub.status.idle":"2024-03-17T12:00:01.639934Z","shell.execute_reply.started":"2024-03-17T11:59:59.360370Z","shell.execute_reply":"2024-03-17T12:00:01.638744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above, we have successfully separated the image files into 2 different subdirectories, `deepfake-faces/Real` and `deepfake-faces/Fake`.","metadata":{}},{"cell_type":"markdown","source":"Next, we will organize the data into their appropriate categories before splitting them into training and test/validation data.\n\nThis is achieved by splitting the images from `deepfake_and_real_images` into their training/validation/test sets first since those have already been organised for us, then adding on the images from `deepfake_faces`.","metadata":{"id":"UFcL8HOQJWsO"}},{"cell_type":"code","source":"TRAIN_PATH = '/kaggle/input/deepfake-and-real-images/Dataset/Train'\nVALIDATION_PATH = '/kaggle/input/deepfake-and-real-images/Dataset/Validation'\nTEST_PATH = '/kaggle/input/deepfake-and-real-images/Dataset/Test'\n\ntrain = tf.keras.utils.image_dataset_from_directory(TRAIN_PATH, labels = 'inferred', image_size=(224,224),)\nval = tf.keras.utils.image_dataset_from_directory(VALIDATION_PATH, labels = 'inferred', image_size=(224,224),)\ntest =  tf.keras.utils.image_dataset_from_directory(TEST_PATH, labels = 'inferred', image_size=(224,224),)\n\nprint(train.class_names)","metadata":{"id":"g2wYPQ-LU61m","execution":{"iopub.status.busy":"2024-03-18T14:39:59.260127Z","iopub.execute_input":"2024-03-18T14:39:59.261080Z","iopub.status.idle":"2024-03-18T14:43:47.783571Z","shell.execute_reply.started":"2024-03-18T14:39:59.261041Z","shell.execute_reply":"2024-03-18T14:43:47.782595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deepfake_faces = tf.keras.utils.image_dataset_from_directory('/kaggle/working/deepfake-faces', labels='inferred', image_size=(224,224),)\n\nprint(deepfake_faces.class_names)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T12:04:33.958454Z","iopub.execute_input":"2024-03-17T12:04:33.959104Z","iopub.status.idle":"2024-03-17T12:04:35.881774Z","shell.execute_reply.started":"2024-03-17T12:04:33.959067Z","shell.execute_reply":"2024-03-17T12:04:35.880868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deepfake_faces = deepfake_faces.shuffle(10, reshuffle_each_iteration=True)\n\ntrain_size = int(0.7 * len(deepfake_faces))\ntest_size = int(0.15 * len(deepfake_faces))\nval_size = int(0.15 * len(deepfake_faces))\n\ntrain2 = deepfake_faces.take(train_size)\ntest2 = deepfake_faces.skip(train_size)\nval2 = test2.skip(val_size)\ntest2 = test2.take(test_size)\n\ntrain_merged = train.concatenate(train2)\nval_merged = val.concatenate(val2)\ntest_merged = test.concatenate(test2)\n\n# print(train_merged.class_names)\nprint(len(train_merged), len(train), len(train2))\nprint(len(train_merged), len(val_merged), len(test_merged))","metadata":{"execution":{"iopub.status.busy":"2024-03-17T12:04:35.884639Z","iopub.execute_input":"2024-03-17T12:04:35.884975Z","iopub.status.idle":"2024-03-17T12:04:35.916432Z","shell.execute_reply.started":"2024-03-17T12:04:35.884947Z","shell.execute_reply":"2024-03-17T12:04:35.915577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are now done with merging our datasets, and can move on to training our model.","metadata":{}},{"cell_type":"markdown","source":"## Training of Model","metadata":{}},{"cell_type":"markdown","source":"### Implementation of Rescaling, Data Augmentation & Callbacks","metadata":{}},{"cell_type":"code","source":"#Rescaling and Resizing\nrescale_and_resize = tf.keras.models.Sequential([\n    tf.keras.layers.Resizing(224,224),\n    tf.keras.layers.Rescaling(1./255)\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_augmentation = tf.keras.models.Sequential([\n    tf.keras.layers.RandomRotation(0.2),\n])","metadata":{"execution":{"iopub.status.busy":"2024-03-18T14:43:47.794987Z","iopub.execute_input":"2024-03-18T14:43:47.795293Z","iopub.status.idle":"2024-03-18T14:43:47.811206Z","shell.execute_reply.started":"2024-03-18T14:43:47.795258Z","shell.execute_reply":"2024-03-18T14:43:47.810418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class CustomCallback(tf.keras.callbacks.Callback):\n#   def on_epoch_end(self, epoch, logs={}):\n#     if(logs.get('val_accuracy') >= 0.75):\n#       print(\"Accuracy>=75%. Cancelling training.\")\n# acc_limit_callback = CustomCallback()\n\n\nCHECKPOINT_PATH = '/kaggle/working/output-models/best_model.keras'\n\ncheckpoint_callback = keras.callbacks.ModelCheckpoint(\n    filepath=CHECKPOINT_PATH,\n    monitor='sparse_categorical_accuracy',\n    mode='max',\n    save_best_only=True)\n\n\ncallback_list = [checkpoint_callback]","metadata":{"execution":{"iopub.status.busy":"2024-03-18T14:43:47.813324Z","iopub.execute_input":"2024-03-18T14:43:47.813594Z","iopub.status.idle":"2024-03-18T14:43:47.820359Z","shell.execute_reply.started":"2024-03-18T14:43:47.813572Z","shell.execute_reply":"2024-03-18T14:43:47.819576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom functions for testing model","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\ndef produce_cm(model, test_dataset):\n    \n    true_labels = []\n    for images, labels in test_dataset.unbatch().batch(1):\n        true_labels.append(labels.numpy())\n\n    test_labels = np.array(true_labels).flatten()\n\n    predictions = model.predict(test_dataset)\n    predicted_classes = predictions.argmax(axis=1)\n    true_classes = test_labels  # Assuming you have these\n\n\n    print(classification_report(true_classes, predicted_classes))\n    \n    cm = confusion_matrix(true_classes, predicted_classes)\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 1', 'Class 2'], yticklabels=['Class 1', 'Class 2'])\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T14:43:47.821402Z","iopub.execute_input":"2024-03-18T14:43:47.821686Z","iopub.status.idle":"2024-03-18T14:43:47.935672Z","shell.execute_reply.started":"2024-03-18T14:43:47.821639Z","shell.execute_reply":"2024-03-18T14:43:47.934926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_accuracy(model, test_dataset):\n    loss, accuracy = model.evaluate(test_dataset)\n    print(\"Test accuracy:\", accuracy)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-18T14:43:47.936838Z","iopub.execute_input":"2024-03-18T14:43:47.937158Z","iopub.status.idle":"2024-03-18T14:43:47.941507Z","shell.execute_reply.started":"2024-03-18T14:43:47.937128Z","shell.execute_reply":"2024-03-18T14:43:47.940701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nfrom skimage import transform\n\ndef load(filename):\n    img = keras.preprocessing.image.load_img(filename, target_size = (224, 224))\n    img = keras.preprocessing.image.img_to_array(img)\n    img = np.expand_dims(img, axis = 0)\n    return img\n\ndef test_image(model, filename):\n    image = load(filename)\n    logits = model.predict(image)\n    probabilities = np.exp(logits) / np.sum(np.exp(logits))\n    print(f'Logits: {logits}')\n    print(f'Probabilities: {probabilities}')\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-18T14:43:47.942904Z","iopub.execute_input":"2024-03-18T14:43:47.943165Z","iopub.status.idle":"2024-03-18T14:43:47.969491Z","shell.execute_reply.started":"2024-03-18T14:43:47.943143Z","shell.execute_reply":"2024-03-18T14:43:47.968699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model implementation","metadata":{}},{"cell_type":"markdown","source":"### Trying to implement our own CNN","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.Input(shape=(224,224,3)),\n    \n    rescale_and_resize,\n    data_augmentation,\n\n    \n    tf.keras.layers.Conv2D(32, kernel_size=5, activation='relu'),\n    tf.keras.layers.Conv2D(32, kernel_size=5,activation='relu'),\n    tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=(2,2)),\n    \n    tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu'),\n    tf.keras.layers.Conv2D(64, kernel_size=3,activation='relu'),\n    tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=(2,2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(2), #there are 2 different classes \n])\n\n\nmodel.compile(optimizer=tf._optimizers.Adam(learning_rate=0.001),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n             )","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:16:52.455945Z","iopub.status.idle":"2024-03-18T13:16:52.456312Z","shell.execute_reply.started":"2024-03-18T13:16:52.456137Z","shell.execute_reply":"2024-03-18T13:16:52.456151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = model.fit(\n#     train_merged,\n#     validation_data=val_merged,\n#     epochs=10,\n#     callbacks=callback_list\n# )","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:16:52.457611Z","iopub.status.idle":"2024-03-18T13:16:52.457998Z","shell.execute_reply.started":"2024-03-18T13:16:52.457804Z","shell.execute_reply":"2024-03-18T13:16:52.457819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For our testing, we handpick a few images from either the test dataset or images that have never been introduced before (sourced online).","metadata":{}},{"cell_type":"code","source":"#idk why this doesnt work lol might be because loading stuff \n\n# def test_model(model_input):\n#     #Screenshot (12) and (13) are both deepfaked images.\n#     files = [\n#         '/kaggle/input/testimage/Screenshot (12).png', \n#         '/kaggle/input/testimage2/Screenshot (13).png', \n#     ]\n    \n#     for i in range(10):\n#         num = str(random.randint(1,4000))\n#         if i%2:\n#             file_name = '/kaggle/input/deepfake-and-real-images/Dataset/Test/Real/real_' + num + '.jpg'\n#         else: \n#             file_name = '/kaggle/input/deepfake-and-real-images/Dataset/Test/Fake/fake_' + num + '.jpg'\n#         files.append(file_name)\n    \n    \n#     for path in files:\n#         image = load('/kaggle/input/testimage/Screenshot (12).png')\n#         logits = model_input.predict(image)\n#         probabilities = np.exp(logits) / np.sum(np.exp(logits))\n#         print(f'{path}: Logits = {logits}, Probabilities = {probabilities}')\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:16:52.459981Z","iopub.status.idle":"2024-03-18T13:16:52.460359Z","shell.execute_reply.started":"2024-03-18T13:16:52.460176Z","shell.execute_reply":"2024-03-18T13:16:52.460192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_image('/kaggle/input/testimage/Screenshot (12).png')","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:16:52.461503Z","iopub.status.idle":"2024-03-18T13:16:52.461880Z","shell.execute_reply.started":"2024-03-18T13:16:52.461696Z","shell.execute_reply":"2024-03-18T13:16:52.461715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_image('/kaggle/input/testimage2/Screenshot (13).png')","metadata":{"execution":{"iopub.status.busy":"2024-03-16T18:48:57.032097Z","iopub.execute_input":"2024-03-16T18:48:57.032831Z","iopub.status.idle":"2024-03-16T18:48:57.106659Z","shell.execute_reply.started":"2024-03-16T18:48:57.032799Z","shell.execute_reply":"2024-03-16T18:48:57.105566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_image('/kaggle/input/deepfake-and-real-images/Dataset/Test/Real/real_412.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-03-16T18:48:58.213873Z","iopub.execute_input":"2024-03-16T18:48:58.214711Z","iopub.status.idle":"2024-03-16T18:48:58.288203Z","shell.execute_reply.started":"2024-03-16T18:48:58.214678Z","shell.execute_reply":"2024-03-16T18:48:58.287202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_image('/kaggle/input/deepfake-and-real-images/Dataset/Test/Fake/fake_1004.jpg')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T18:49:12.808267Z","iopub.execute_input":"2024-03-16T18:49:12.808921Z","iopub.status.idle":"2024-03-16T18:49:12.885862Z","shell.execute_reply.started":"2024-03-16T18:49:12.808890Z","shell.execute_reply":"2024-03-16T18:49:12.884946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trying InceptionV3 with `deepfake-and-real-images` dataset only","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\n\ndef preprocess(image, label):\n    image = preprocess_input(image)\n    return image, label\n\ntrain_preprocessed = train.map(preprocess)\nval_preprocessed = val.map(preprocess)\ntest_preprocessed = test.map(preprocess)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T14:43:47.970567Z","iopub.execute_input":"2024-03-18T14:43:47.971196Z","iopub.status.idle":"2024-03-18T14:43:52.609921Z","shell.execute_reply.started":"2024-03-18T14:43:47.971163Z","shell.execute_reply":"2024-03-18T14:43:52.609147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224,224,3))\nbase_model.trainable = False\n\nmodel_inception = tf.keras.models.Sequential([\n    tf.keras.Input(shape=(224,224,3)),\n    \n    rescale_and_resize,\n    data_augmentation,\n\n    base_model,\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(2, activation='softmax'), #there are 2 different classes \n])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_inception.compile(optimizer=tf._optimizers.Adam(learning_rate=0.001),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics=['sparse_categorical_accuracy']\n             )","metadata":{"execution":{"iopub.status.busy":"2024-03-18T14:43:52.611026Z","iopub.execute_input":"2024-03-18T14:43:52.611331Z","iopub.status.idle":"2024-03-18T14:43:52.625604Z","shell.execute_reply.started":"2024-03-18T14:43:52.611307Z","shell.execute_reply":"2024-03-18T14:43:52.624857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_real = model_inception.fit(\n    train_preprocessed,\n    validation_data=val_preprocessed,\n    epochs=10\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T14:43:52.695609Z","iopub.execute_input":"2024-03-18T14:43:52.695902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy(model_inception, test_preprocessed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"produce_cm(model_inception, test_processed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that the current model is overfitting. Hence we perform hyperparameter tuning.\n\nFirst we decrease the learning rate of our model.\n","metadata":{}},{"cell_type":"code","source":"#HYPERPARAMETER TUNING CODE HERE","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trying InceptionV3 with merged dataset","metadata":{}},{"cell_type":"code","source":"train_merged_preprocessed = train_merged.map(preprocess)\nval_merged_prerocessed = val_merged.map(preprocess)\ntest_merged_preprocessed = test_merged.map(preprocess)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model2 = InceptionV3(weights='imagenet', include_top=False, input_shape=(224,224,3))\nbase_model2.trainable = False\n\nmodel_inception2 = tf.keras.models.Sequential([\n    tf.keras.Input(shape=(224,224,3)),\n    \n    rescale_and_resize,\n    data_augmentation,\n\n    base_model2,\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(2, activation='softmax'), #there are 2 different classes \n])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_inception2 = model_inception2.fit(\n    train_merged_preprocessed,\n    validation_data=val_merged_preprocessed,\n    epochs=10,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T07:45:01.184324Z","iopub.execute_input":"2024-03-17T07:45:01.184618Z","iopub.status.idle":"2024-03-17T09:16:30.452921Z","shell.execute_reply.started":"2024-03-17T07:45:01.184592Z","shell.execute_reply":"2024-03-17T09:16:30.451973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy(model_inception2, test_merged_preprocessed)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T06:09:09.030156Z","iopub.status.idle":"2024-03-18T06:09:09.030475Z","shell.execute_reply.started":"2024-03-18T06:09:09.030318Z","shell.execute_reply":"2024-03-18T06:09:09.030331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trying ResNet model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50\n\nbase_model_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:38:07.056031Z","iopub.execute_input":"2024-03-17T13:38:07.056332Z","iopub.status.idle":"2024-03-17T13:38:08.988615Z","shell.execute_reply.started":"2024-03-17T13:38:07.056296Z","shell.execute_reply":"2024-03-17T13:38:08.987866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model_resnet.trainable = False\n\nmodel_resnet = tf.keras.models.Sequential([\n    tf.keras.Input(shape=(224,224,3)),\n    \n    rescale_and_resize,\n    data_augmentation,\n\n    base_model_resnet,\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid'),\n])\n\n\nmodel_resnet.compile(optimizer=tf._optimizers.Adam(learning_rate=0.001),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy']\n             )","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:38:08.989848Z","iopub.execute_input":"2024-03-17T13:38:08.990109Z","iopub.status.idle":"2024-03-17T13:38:09.025919Z","shell.execute_reply.started":"2024-03-17T13:38:08.990086Z","shell.execute_reply":"2024-03-17T13:38:09.025035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_resnet = model_resnet.fit(\n    train_merged,\n    validation_data=val_merged,\n    epochs=10,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:38:09.027953Z","iopub.execute_input":"2024-03-17T13:38:09.028294Z","iopub.status.idle":"2024-03-17T16:00:46.392647Z","shell.execute_reply.started":"2024-03-17T13:38:09.028262Z","shell.execute_reply":"2024-03-17T16:00:46.391849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_resnet, accuracy_resnet = model_resnet.evaluate(test_merged)\nprint(\"Test accuracy:\", accuracy_resnet)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T16:23:14.647178Z","iopub.execute_input":"2024-03-17T16:23:14.647885Z","iopub.status.idle":"2024-03-17T16:24:19.013545Z","shell.execute_reply.started":"2024-03-17T16:23:14.647849Z","shell.execute_reply":"2024-03-17T16:24:19.012675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the accuracy of our model, we can conclude that ResNet is significantly less accurate than InceptionV3 (pre-hyperparameter tuning), hence we will continue using InceptionV3 instead.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}